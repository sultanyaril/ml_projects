{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf18f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import flappy_bird_gym\n",
    "import gymnasium\n",
    "env = flappy_bird_gym.make(\"FlappyBird-12-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89606d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "def build_model(obs, actions):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(64, activation='relu', input_shape=(1, obs)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435fcd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444c85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22ba89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=0.1, value_min=.0001, value_test=.0, nb_steps=4000000)\n",
    "    memory = SequentialMemory(limit=100000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                enable_dueling_network=True, dueling_type='avg',\n",
    "                nb_actions=actions, nb_steps_warmup=500)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f896e74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1, 64)             832       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 128)            8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 256)            33024     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1, 64)             16448     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1, 128)            8320      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,202\n",
      "Trainable params: 67,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(obs, actions)\n",
    "dqn = build_agent(model, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317f4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7afb3cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "#Training the Neural Network\n",
    "dqn.compile(Adam(learning_rate=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41620de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 8000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: -10.3000\n",
      "103 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 4730.556 - mean_q: -9.884 - mean_eps: 0.100 - score: 0.000\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 1869.106 - mean_q: -20.883 - mean_eps: 0.100 - score: 0.000\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 388.150 - mean_q: -29.599 - mean_eps: 0.099 - score: 0.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: -10.0000\n",
      "100 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 100.680 - mean_q: -37.899 - mean_eps: 0.099 - score: 0.000\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 74.232 - mean_q: -49.085 - mean_eps: 0.099 - score: 0.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 62.137 - mean_q: -57.860 - mean_eps: 0.099 - score: 0.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 57.082 - mean_q: -66.351 - mean_eps: 0.098 - score: 0.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 37.478 - mean_q: -73.263 - mean_eps: 0.098 - score: 0.000\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 69s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 45.828 - mean_q: -81.761 - mean_eps: 0.098 - score: 0.000\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 42.661 - mean_q: -90.744 - mean_eps: 0.098 - score: 0.000\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 29.793 - mean_q: -99.424 - mean_eps: 0.097 - score: 0.000\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 25.398 - mean_q: -107.791 - mean_eps: 0.097 - score: 0.000\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 25.698 - mean_q: -119.363 - mean_eps: 0.097 - score: 0.000\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 18.322 - mean_q: -127.024 - mean_eps: 0.097 - score: 0.000\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 12.893 - mean_q: -136.684 - mean_eps: 0.096 - score: 0.000\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 19.357 - mean_q: -142.975 - mean_eps: 0.096 - score: 0.000\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 16.251 - mean_q: -150.332 - mean_eps: 0.096 - score: 0.000\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 69s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 13.591 - mean_q: -159.036 - mean_eps: 0.096 - score: 0.000\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 9.521 - mean_q: -168.071 - mean_eps: 0.095 - score: 0.000\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 6.543 - mean_q: -173.622 - mean_eps: 0.095 - score: 0.000\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 7.318 - mean_q: -182.401 - mean_eps: 0.095 - score: 0.000\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 5.158 - mean_q: -189.883 - mean_eps: 0.095 - score: 0.000\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 96s 10ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 5.970 - mean_q: -198.233 - mean_eps: 0.094 - score: 0.000\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 94s 9ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 4.628 - mean_q: -205.189 - mean_eps: 0.094 - score: 0.000\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      " 8197/10000 [=======================>......] - ETA: 17s - reward: -9.8817"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 10682s 1s/step - reward: -7.8000\n",
      "78 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 294.673 - mean_q: -470.085 - mean_eps: 0.069 - score: 0.231\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 559s 56ms/step - reward: -8.1000\n",
      "81 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 306.240 - mean_q: -466.172 - mean_eps: 0.068 - score: 0.207\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: -8.2000\n",
      "82 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 280.941 - mean_q: -464.115 - mean_eps: 0.068 - score: 0.170\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 226s 23ms/step - reward: -7.5000\n",
      "75 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 273.791 - mean_q: -463.444 - mean_eps: 0.068 - score: 0.298\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: -7.5000\n",
      "75 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 264.585 - mean_q: -461.993 - mean_eps: 0.068 - score: 0.329\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: -7.5000\n",
      "75 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 261.792 - mean_q: -460.708 - mean_eps: 0.067 - score: 0.377\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: -7.4000\n",
      "74 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 240.238 - mean_q: -457.763 - mean_eps: 0.067 - score: 0.308\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: -7.7000\n",
      "77 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 228.264 - mean_q: -455.314 - mean_eps: 0.067 - score: 0.282\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: -7.6000\n",
      "76 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 215.555 - mean_q: -453.044 - mean_eps: 0.067 - score: 0.277\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: -7.5000\n",
      "75 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 214.143 - mean_q: -452.096 - mean_eps: 0.066 - score: 0.231\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: -7.9000\n",
      "79 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 200.585 - mean_q: -448.397 - mean_eps: 0.066 - score: 0.223\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: -7.3000\n",
      "73 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 166.676 - mean_q: -450.285 - mean_eps: 0.066 - score: 0.334\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: -7.0000\n",
      "70 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 174.334 - mean_q: -449.175 - mean_eps: 0.066 - score: 0.387\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: -7.6000\n",
      "76 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 179.893 - mean_q: -448.054 - mean_eps: 0.065 - score: 0.317\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      " 6590/10000 [==================>...........] - ETA: 1:15 - reward: -7.7390"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 37295s 4s/step - reward: -8.3000\n",
      "83 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 158.554 - mean_q: -396.524 - mean_eps: 0.059 - score: 0.201\n",
      "\n",
      "Interval 164 (1630000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -8.0000\n",
      "80 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 151.463 - mean_q: -400.513 - mean_eps: 0.059 - score: 0.274\n",
      "\n",
      "Interval 165 (1640000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: -8.3000\n",
      "83 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 158.737 - mean_q: -402.610 - mean_eps: 0.059 - score: 0.185\n",
      "\n",
      "Interval 166 (1650000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -8.0000\n",
      "80 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 149.376 - mean_q: -403.531 - mean_eps: 0.059 - score: 0.209\n",
      "\n",
      "Interval 167 (1660000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -8.3000\n",
      "83 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 141.368 - mean_q: -403.680 - mean_eps: 0.058 - score: 0.234\n",
      "\n",
      "Interval 168 (1670000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -8.2000\n",
      "82 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 138.695 - mean_q: -399.760 - mean_eps: 0.058 - score: 0.249\n",
      "\n",
      "Interval 169 (1680000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -8.0000\n",
      "80 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 126.045 - mean_q: -399.834 - mean_eps: 0.058 - score: 0.321\n",
      "\n",
      "Interval 170 (1690000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -8.7000\n",
      "87 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 127.066 - mean_q: -399.578 - mean_eps: 0.058 - score: 0.130\n",
      "\n",
      "Interval 171 (1700000 steps performed)\n",
      "10000/10000 [==============================] - 75s 7ms/step - reward: -7.8000\n",
      "78 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 138.216 - mean_q: -396.392 - mean_eps: 0.057 - score: 0.243\n",
      "\n",
      "Interval 172 (1710000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -7.9000\n",
      "79 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 153.745 - mean_q: -393.353 - mean_eps: 0.057 - score: 0.273\n",
      "\n",
      "Interval 173 (1720000 steps performed)\n",
      "10000/10000 [==============================] - 69s 7ms/step - reward: -8.1000\n",
      "81 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 140.822 - mean_q: -390.415 - mean_eps: 0.057 - score: 0.200\n",
      "\n",
      "Interval 174 (1730000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -7.1000\n",
      "71 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 143.876 - mean_q: -384.725 - mean_eps: 0.057 - score: 0.500\n",
      "\n",
      "Interval 175 (1740000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: -7.0000\n",
      "70 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 145.314 - mean_q: -380.184 - mean_eps: 0.056 - score: 0.462\n",
      "\n",
      "Interval 176 (1750000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -6.9000\n",
      "69 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 156.501 - mean_q: -377.672 - mean_eps: 0.056 - score: 0.498\n",
      "\n",
      "Interval 177 (1760000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -7.2000\n",
      "72 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 167.144 - mean_q: -373.510 - mean_eps: 0.056 - score: 0.446\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: -6.8000\n",
      "68 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 155.735 - mean_q: -368.722 - mean_eps: 0.056 - score: 0.628\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -7.1000\n",
      "71 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 159.970 - mean_q: -362.114 - mean_eps: 0.055 - score: 0.401\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -6.8000\n",
      "68 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 160.071 - mean_q: -358.835 - mean_eps: 0.055 - score: 0.568\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -7.5000\n",
      "75 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 166.805 - mean_q: -355.231 - mean_eps: 0.055 - score: 0.372\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -7.0000\n",
      "70 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 172.179 - mean_q: -348.777 - mean_eps: 0.055 - score: 0.539\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -6.8000\n",
      "68 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 135.301 - mean_q: -350.812 - mean_eps: 0.054 - score: 0.691\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -7.2000\n",
      "72 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 147.899 - mean_q: -352.252 - mean_eps: 0.054 - score: 0.439\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -7.2000\n",
      "72 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 148.027 - mean_q: -352.533 - mean_eps: 0.054 - score: 0.442\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -7.4000\n",
      "74 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 159.381 - mean_q: -359.026 - mean_eps: 0.054 - score: 0.516\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -6.9000\n",
      "69 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 189.884 - mean_q: -358.160 - mean_eps: 0.053 - score: 0.534\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: -7.0000\n",
      "70 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 151.276 - mean_q: -353.480 - mean_eps: 0.053 - score: 0.559\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n",
      "10000/10000 [==============================] - 82s 8ms/step - reward: -7.1000\n",
      "71 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 169.683 - mean_q: -352.141 - mean_eps: 0.053 - score: 0.479\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -7.3000\n",
      "73 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 177.551 - mean_q: -355.279 - mean_eps: 0.053 - score: 0.557\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -7.2000\n",
      "72 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 166.854 - mean_q: -353.547 - mean_eps: 0.052 - score: 0.531\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 87s 9ms/step - reward: -6.9000\n",
      "69 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 160.969 - mean_q: -352.912 - mean_eps: 0.052 - score: 0.648\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 83s 8ms/step - reward: -6.7000\n",
      "67 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 178.508 - mean_q: -352.313 - mean_eps: 0.052 - score: 0.730\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -7.2000\n",
      "72 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 174.409 - mean_q: -347.063 - mean_eps: 0.052 - score: 0.539\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -6.8000\n",
      "68 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 215.375 - mean_q: -346.943 - mean_eps: 0.051 - score: 0.567\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 81s 8ms/step - reward: -7.7000\n",
      "77 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 242.056 - mean_q: -343.329 - mean_eps: 0.051 - score: 0.319\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: -6.4000\n",
      "64 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 222.746 - mean_q: -341.653 - mean_eps: 0.051 - score: 0.769\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: -6.2000\n",
      "62 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 210.530 - mean_q: -339.478 - mean_eps: 0.051 - score: 0.726\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -7.3000\n",
      "73 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 211.806 - mean_q: -338.442 - mean_eps: 0.050 - score: 0.417\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: -7.2000\n",
      "72 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 205.983 - mean_q: -335.472 - mean_eps: 0.050 - score: 0.525\n",
      "\n",
      "Interval 201 (2000000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: -6.5000\n",
      "65 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 217.539 - mean_q: -333.790 - mean_eps: 0.050 - score: 0.688\n",
      "\n",
      "Interval 202 (2010000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -6.9000\n",
      "69 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 196.275 - mean_q: -331.926 - mean_eps: 0.050 - score: 0.646\n",
      "\n",
      "Interval 203 (2020000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -6.8000\n",
      "68 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 178.627 - mean_q: -326.355 - mean_eps: 0.049 - score: 0.493\n",
      "\n",
      "Interval 204 (2030000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -6.3000\n",
      "63 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 184.214 - mean_q: -325.393 - mean_eps: 0.049 - score: 0.692\n",
      "\n",
      "Interval 205 (2040000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -6.8000\n",
      "68 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 172.658 - mean_q: -323.846 - mean_eps: 0.049 - score: 0.521\n",
      "\n",
      "Interval 206 (2050000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -6.0000\n",
      "60 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 154.485 - mean_q: -324.867 - mean_eps: 0.049 - score: 0.766\n",
      "\n",
      "Interval 207 (2060000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -6.1000\n",
      "61 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 166.697 - mean_q: -321.432 - mean_eps: 0.048 - score: 0.667\n",
      "\n",
      "Interval 208 (2070000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -6.0000\n",
      "60 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 179.065 - mean_q: -319.615 - mean_eps: 0.048 - score: 0.791\n",
      "\n",
      "Interval 209 (2080000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -6.6000\n",
      "66 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 168.000 - mean_q: -316.405 - mean_eps: 0.048 - score: 0.795\n",
      "\n",
      "Interval 210 (2090000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -6.0000\n",
      "60 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 172.999 - mean_q: -313.343 - mean_eps: 0.048 - score: 0.795\n",
      "\n",
      "Interval 211 (2100000 steps performed)\n",
      "10000/10000 [==============================] - 87s 9ms/step - reward: -6.5000\n",
      "65 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 177.964 - mean_q: -310.163 - mean_eps: 0.047 - score: 0.642\n",
      "\n",
      "Interval 212 (2110000 steps performed)\n",
      "10000/10000 [==============================] - 83s 8ms/step - reward: -6.4000\n",
      "64 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 187.247 - mean_q: -306.819 - mean_eps: 0.047 - score: 0.720\n",
      "\n",
      "Interval 213 (2120000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -6.1000\n",
      "61 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 177.830 - mean_q: -301.177 - mean_eps: 0.047 - score: 0.931\n",
      "\n",
      "Interval 214 (2130000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -6.3000\n",
      "63 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 182.105 - mean_q: -298.423 - mean_eps: 0.047 - score: 0.728\n",
      "\n",
      "Interval 215 (2140000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -6.0000\n",
      "60 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 192.506 - mean_q: -297.474 - mean_eps: 0.046 - score: 0.825\n",
      "\n",
      "Interval 216 (2150000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -6.2000\n",
      "62 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 203.623 - mean_q: -295.448 - mean_eps: 0.046 - score: 0.728\n",
      "\n",
      "Interval 217 (2160000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -6.4000\n",
      "64 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 220.368 - mean_q: -293.968 - mean_eps: 0.046 - score: 0.574\n",
      "\n",
      "Interval 218 (2170000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -6.5000\n",
      "65 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 226.369 - mean_q: -293.858 - mean_eps: 0.046 - score: 0.589\n",
      "\n",
      "Interval 219 (2180000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -6.4000\n",
      "64 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 244.520 - mean_q: -291.474 - mean_eps: 0.045 - score: 0.659\n",
      "\n",
      "Interval 220 (2190000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -6.1000\n",
      "61 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 195.505 - mean_q: -295.133 - mean_eps: 0.045 - score: 0.855\n",
      "\n",
      "Interval 221 (2200000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -6.6000\n",
      "66 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 222.544 - mean_q: -287.945 - mean_eps: 0.045 - score: 0.586\n",
      "\n",
      "Interval 222 (2210000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -6.5000\n",
      "65 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 203.564 - mean_q: -284.184 - mean_eps: 0.045 - score: 0.544\n",
      "\n",
      "Interval 223 (2220000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -5.7000\n",
      "57 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 152.538 - mean_q: -281.541 - mean_eps: 0.044 - score: 0.951\n",
      "\n",
      "Interval 224 (2230000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -7.1000\n",
      "71 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 168.589 - mean_q: -279.394 - mean_eps: 0.044 - score: 0.557\n",
      "\n",
      "Interval 225 (2240000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: -6.1000\n",
      "61 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 155.101 - mean_q: -283.387 - mean_eps: 0.044 - score: 0.861\n",
      "\n",
      "Interval 226 (2250000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -6.0000\n",
      "60 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 168.299 - mean_q: -281.578 - mean_eps: 0.044 - score: 0.872\n",
      "\n",
      "Interval 227 (2260000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -5.6000\n",
      "56 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 162.529 - mean_q: -278.530 - mean_eps: 0.043 - score: 1.177\n",
      "\n",
      "Interval 228 (2270000 steps performed)\n",
      "10000/10000 [==============================] - 75s 7ms/step - reward: -5.9000\n",
      "59 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 159.465 - mean_q: -274.290 - mean_eps: 0.043 - score: 0.816\n",
      "\n",
      "Interval 229 (2280000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 75s 8ms/step - reward: -6.5000\n",
      "65 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 173.870 - mean_q: -273.459 - mean_eps: 0.043 - score: 0.582\n",
      "\n",
      "Interval 230 (2290000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: -6.0000\n",
      "60 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 183.085 - mean_q: -266.782 - mean_eps: 0.043 - score: 0.845\n",
      "\n",
      "Interval 231 (2300000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -6.4000\n",
      "64 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 177.881 - mean_q: -263.674 - mean_eps: 0.042 - score: 0.837\n",
      "\n",
      "Interval 232 (2310000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: -6.0000\n",
      "60 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 178.729 - mean_q: -259.130 - mean_eps: 0.042 - score: 0.823\n",
      "\n",
      "Interval 233 (2320000 steps performed)\n",
      "10000/10000 [==============================] - 75s 7ms/step - reward: -6.3000\n",
      "63 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 176.167 - mean_q: -257.499 - mean_eps: 0.042 - score: 0.802\n",
      "\n",
      "Interval 234 (2330000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -5.8000\n",
      "58 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 161.626 - mean_q: -251.356 - mean_eps: 0.042 - score: 1.015\n",
      "\n",
      "Interval 235 (2340000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -6.2000\n",
      "62 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 177.002 - mean_q: -249.201 - mean_eps: 0.041 - score: 0.946\n",
      "\n",
      "Interval 236 (2350000 steps performed)\n",
      "10000/10000 [==============================] - 93s 9ms/step - reward: -6.7000\n",
      "67 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 154.060 - mean_q: -247.967 - mean_eps: 0.041 - score: 0.542\n",
      "\n",
      "Interval 237 (2360000 steps performed)\n",
      "10000/10000 [==============================] - 82s 8ms/step - reward: -6.3000\n",
      "63 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 165.063 - mean_q: -245.070 - mean_eps: 0.041 - score: 0.554\n",
      "\n",
      "Interval 238 (2370000 steps performed)\n",
      "10000/10000 [==============================] - 92s 9ms/step - reward: -6.1000\n",
      "61 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 134.065 - mean_q: -243.936 - mean_eps: 0.041 - score: 0.762\n",
      "\n",
      "Interval 239 (2380000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: -5.9000\n",
      "59 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 156.807 - mean_q: -241.967 - mean_eps: 0.040 - score: 0.910\n",
      "\n",
      "Interval 240 (2390000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -6.3000\n",
      "63 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 152.178 - mean_q: -240.283 - mean_eps: 0.040 - score: 0.976\n",
      "\n",
      "Interval 241 (2400000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -6.1000\n",
      "61 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 156.279 - mean_q: -238.200 - mean_eps: 0.040 - score: 0.956\n",
      "\n",
      "Interval 242 (2410000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: -6.3000\n",
      "63 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 133.040 - mean_q: -235.826 - mean_eps: 0.040 - score: 0.791\n",
      "\n",
      "Interval 243 (2420000 steps performed)\n",
      "10000/10000 [==============================] - 86s 9ms/step - reward: -6.5000\n",
      "65 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 124.921 - mean_q: -233.850 - mean_eps: 0.039 - score: 0.643\n",
      "\n",
      "Interval 244 (2430000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -6.1000\n",
      "61 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 168.694 - mean_q: -232.105 - mean_eps: 0.039 - score: 0.754\n",
      "\n",
      "Interval 245 (2440000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -6.5000\n",
      "65 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 158.079 - mean_q: -229.176 - mean_eps: 0.039 - score: 0.703\n",
      "\n",
      "Interval 246 (2450000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -6.4000\n",
      "64 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 146.468 - mean_q: -227.814 - mean_eps: 0.039 - score: 0.902\n",
      "\n",
      "Interval 247 (2460000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: -6.1000\n",
      "61 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 166.279 - mean_q: -226.322 - mean_eps: 0.038 - score: 0.799\n",
      "\n",
      "Interval 248 (2470000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -6.7000\n",
      "67 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 181.991 - mean_q: -224.495 - mean_eps: 0.038 - score: 0.644\n",
      "\n",
      "Interval 249 (2480000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -6.1000\n",
      "61 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 205.637 - mean_q: -225.267 - mean_eps: 0.038 - score: 1.031\n",
      "\n",
      "Interval 250 (2490000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -6.3000\n",
      "63 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 209.704 - mean_q: -220.487 - mean_eps: 0.038 - score: 0.886\n",
      "\n",
      "Interval 251 (2500000 steps performed)\n",
      "10000/10000 [==============================] - 75s 7ms/step - reward: -6.6000\n",
      "66 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 203.188 - mean_q: -216.466 - mean_eps: 0.037 - score: 0.853\n",
      "\n",
      "Interval 252 (2510000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -6.4000\n",
      "64 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 233.636 - mean_q: -214.614 - mean_eps: 0.037 - score: 0.943\n",
      "\n",
      "Interval 253 (2520000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -6.2000\n",
      "62 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 217.455 - mean_q: -213.451 - mean_eps: 0.037 - score: 0.824\n",
      "\n",
      "Interval 254 (2530000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -6.1000\n",
      "61 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 208.018 - mean_q: -213.457 - mean_eps: 0.037 - score: 0.960\n",
      "\n",
      "Interval 255 (2540000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -6.2000\n",
      "62 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 246.155 - mean_q: -209.987 - mean_eps: 0.036 - score: 0.944\n",
      "\n",
      "Interval 256 (2550000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -6.1000\n",
      "61 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 236.921 - mean_q: -212.223 - mean_eps: 0.036 - score: 0.923\n",
      "\n",
      "Interval 257 (2560000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -5.2000\n",
      "52 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 228.428 - mean_q: -210.489 - mean_eps: 0.036 - score: 1.550\n",
      "\n",
      "Interval 258 (2570000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -5.9000\n",
      "59 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 195.494 - mean_q: -208.315 - mean_eps: 0.036 - score: 0.849\n",
      "\n",
      "Interval 259 (2580000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -5.8000\n",
      "58 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 227.659 - mean_q: -211.675 - mean_eps: 0.035 - score: 1.170\n",
      "\n",
      "Interval 260 (2590000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -5.4000\n",
      "54 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 199.612 - mean_q: -211.277 - mean_eps: 0.035 - score: 1.316\n",
      "\n",
      "Interval 261 (2600000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -5.7000\n",
      "57 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 187.564 - mean_q: -205.345 - mean_eps: 0.035 - score: 1.014\n",
      "\n",
      "Interval 262 (2610000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 72s 7ms/step - reward: -5.6000\n",
      "56 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 174.334 - mean_q: -204.250 - mean_eps: 0.035 - score: 1.429\n",
      "\n",
      "Interval 263 (2620000 steps performed)\n",
      "10000/10000 [==============================] - 75s 7ms/step - reward: -5.9000\n",
      "59 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 173.686 - mean_q: -203.495 - mean_eps: 0.034 - score: 1.136\n",
      "\n",
      "Interval 264 (2630000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -5.4000\n",
      "54 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 160.678 - mean_q: -201.502 - mean_eps: 0.034 - score: 1.261\n",
      "\n",
      "Interval 265 (2640000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -5.5000\n",
      "55 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 162.727 - mean_q: -200.650 - mean_eps: 0.034 - score: 1.217\n",
      "\n",
      "Interval 266 (2650000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: -5.4000\n",
      "54 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 151.344 - mean_q: -196.924 - mean_eps: 0.034 - score: 1.109\n",
      "\n",
      "Interval 267 (2660000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: -4.8000\n",
      "48 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 141.288 - mean_q: -192.735 - mean_eps: 0.033 - score: 1.631\n",
      "\n",
      "Interval 268 (2670000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -5.2000\n",
      "52 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 158.110 - mean_q: -188.266 - mean_eps: 0.033 - score: 1.160\n",
      "\n",
      "Interval 269 (2680000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -5.2000\n",
      "52 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 175.514 - mean_q: -183.360 - mean_eps: 0.033 - score: 1.228\n",
      "\n",
      "Interval 270 (2690000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -5.4000\n",
      "54 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 178.475 - mean_q: -182.031 - mean_eps: 0.033 - score: 1.377\n",
      "\n",
      "Interval 271 (2700000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -6.0000\n",
      "60 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 160.131 - mean_q: -178.792 - mean_eps: 0.032 - score: 1.050\n",
      "\n",
      "Interval 272 (2710000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -4.9000\n",
      "49 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 180.142 - mean_q: -177.892 - mean_eps: 0.032 - score: 1.180\n",
      "\n",
      "Interval 273 (2720000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -5.0000\n",
      "50 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 163.078 - mean_q: -172.466 - mean_eps: 0.032 - score: 1.637\n",
      "\n",
      "Interval 274 (2730000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -6.1000\n",
      "61 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 226.144 - mean_q: -170.139 - mean_eps: 0.032 - score: 0.817\n",
      "\n",
      "Interval 275 (2740000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: -5.9000\n",
      "59 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 223.932 - mean_q: -172.700 - mean_eps: 0.031 - score: 1.299\n",
      "\n",
      "Interval 276 (2750000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -5.7000\n",
      "57 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 268.067 - mean_q: -170.724 - mean_eps: 0.031 - score: 1.341\n",
      "\n",
      "Interval 277 (2760000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -5.0000\n",
      "50 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 228.856 - mean_q: -169.272 - mean_eps: 0.031 - score: 1.671\n",
      "\n",
      "Interval 278 (2770000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -4.9000\n",
      "49 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 246.004 - mean_q: -167.453 - mean_eps: 0.031 - score: 1.595\n",
      "\n",
      "Interval 279 (2780000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -4.4000\n",
      "44 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 275.581 - mean_q: -166.801 - mean_eps: 0.030 - score: 1.963\n",
      "\n",
      "Interval 280 (2790000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: -4.6000\n",
      "46 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 251.745 - mean_q: -164.861 - mean_eps: 0.030 - score: 1.763\n",
      "\n",
      "Interval 281 (2800000 steps performed)\n",
      "10000/10000 [==============================] - 82s 8ms/step - reward: -4.7000\n",
      "47 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 255.982 - mean_q: -166.190 - mean_eps: 0.030 - score: 1.565\n",
      "\n",
      "Interval 282 (2810000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: -4.5000\n",
      "45 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 258.430 - mean_q: -163.580 - mean_eps: 0.030 - score: 2.076\n",
      "\n",
      "Interval 283 (2820000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: -4.8000\n",
      "48 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 240.843 - mean_q: -161.030 - mean_eps: 0.029 - score: 2.096\n",
      "\n",
      "Interval 284 (2830000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -4.7000\n",
      "47 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 212.811 - mean_q: -158.411 - mean_eps: 0.029 - score: 1.659\n",
      "\n",
      "Interval 285 (2840000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -4.2000\n",
      "42 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 218.677 - mean_q: -151.870 - mean_eps: 0.029 - score: 1.834\n",
      "\n",
      "Interval 286 (2850000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -4.3000\n",
      "43 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 192.211 - mean_q: -148.226 - mean_eps: 0.029 - score: 1.691\n",
      "\n",
      "Interval 287 (2860000 steps performed)\n",
      "10000/10000 [==============================] - 75s 7ms/step - reward: -5.1000\n",
      "51 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 182.357 - mean_q: -147.338 - mean_eps: 0.028 - score: 1.445\n",
      "\n",
      "Interval 288 (2870000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: -4.1000\n",
      "41 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 187.513 - mean_q: -144.713 - mean_eps: 0.028 - score: 2.080\n",
      "\n",
      "Interval 289 (2880000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -4.8000\n",
      "48 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 209.571 - mean_q: -143.916 - mean_eps: 0.028 - score: 1.533\n",
      "\n",
      "Interval 290 (2890000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -4.4000\n",
      "44 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 205.560 - mean_q: -141.864 - mean_eps: 0.028 - score: 2.364\n",
      "\n",
      "Interval 291 (2900000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -4.9000\n",
      "49 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 240.416 - mean_q: -141.981 - mean_eps: 0.027 - score: 1.316\n",
      "\n",
      "Interval 292 (2910000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -4.5000\n",
      "45 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 188.927 - mean_q: -141.580 - mean_eps: 0.027 - score: 1.638\n",
      "\n",
      "Interval 293 (2920000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: -5.2000\n",
      "52 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 174.671 - mean_q: -140.316 - mean_eps: 0.027 - score: 0.984\n",
      "\n",
      "Interval 294 (2930000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -4.7000\n",
      "47 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 189.461 - mean_q: -140.311 - mean_eps: 0.027 - score: 1.764\n",
      "\n",
      "Interval 295 (2940000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 85s 8ms/step - reward: -5.7000\n",
      "57 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 221.905 - mean_q: -138.321 - mean_eps: 0.026 - score: 1.388\n",
      "\n",
      "Interval 296 (2950000 steps performed)\n",
      "10000/10000 [==============================] - 98s 10ms/step - reward: -5.0000\n",
      "50 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 221.887 - mean_q: -138.668 - mean_eps: 0.026 - score: 1.735\n",
      "\n",
      "Interval 297 (2960000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: -4.6000\n",
      "46 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 222.671 - mean_q: -136.545 - mean_eps: 0.026 - score: 1.872\n",
      "\n",
      "Interval 298 (2970000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -4.4000\n",
      "44 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 220.174 - mean_q: -136.039 - mean_eps: 0.026 - score: 1.809\n",
      "\n",
      "Interval 299 (2980000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -4.9000\n",
      "49 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 200.907 - mean_q: -136.418 - mean_eps: 0.025 - score: 1.468\n",
      "\n",
      "Interval 300 (2990000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -4.0000\n",
      "40 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 190.672 - mean_q: -134.052 - mean_eps: 0.025 - score: 3.087\n",
      "\n",
      "Interval 301 (3000000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -4.2000\n",
      "42 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 187.312 - mean_q: -130.863 - mean_eps: 0.025 - score: 2.382\n",
      "\n",
      "Interval 302 (3010000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -4.5000\n",
      "45 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 210.399 - mean_q: -130.319 - mean_eps: 0.025 - score: 1.718\n",
      "\n",
      "Interval 303 (3020000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: -3.8000\n",
      "38 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 209.656 - mean_q: -127.318 - mean_eps: 0.024 - score: 2.538\n",
      "\n",
      "Interval 304 (3030000 steps performed)\n",
      "10000/10000 [==============================] - 93s 9ms/step - reward: -5.2000\n",
      "52 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 232.596 - mean_q: -124.481 - mean_eps: 0.024 - score: 1.345\n",
      "\n",
      "Interval 305 (3040000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: -4.3000\n",
      "43 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 199.300 - mean_q: -121.908 - mean_eps: 0.024 - score: 1.596\n",
      "\n",
      "Interval 306 (3050000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: -4.7000\n",
      "47 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 178.946 - mean_q: -120.015 - mean_eps: 0.024 - score: 1.774\n",
      "\n",
      "Interval 307 (3060000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -4.8000\n",
      "48 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 170.767 - mean_q: -119.491 - mean_eps: 0.023 - score: 1.730\n",
      "\n",
      "Interval 308 (3070000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -4.1000\n",
      "41 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 169.273 - mean_q: -118.149 - mean_eps: 0.023 - score: 2.438\n",
      "\n",
      "Interval 309 (3080000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -4.8000\n",
      "48 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 147.898 - mean_q: -116.423 - mean_eps: 0.023 - score: 1.575\n",
      "\n",
      "Interval 310 (3090000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -4.1000\n",
      "41 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 187.392 - mean_q: -115.560 - mean_eps: 0.023 - score: 4.219\n",
      "\n",
      "Interval 311 (3100000 steps performed)\n",
      "10000/10000 [==============================] - 93s 9ms/step - reward: -3.8000\n",
      "38 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 180.376 - mean_q: -113.975 - mean_eps: 0.022 - score: 2.438\n",
      "\n",
      "Interval 312 (3110000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: -5.1000\n",
      "51 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 215.392 - mean_q: -109.228 - mean_eps: 0.022 - score: 2.205\n",
      "\n",
      "Interval 313 (3120000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -5.0000\n",
      "50 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 193.003 - mean_q: -106.308 - mean_eps: 0.022 - score: 1.520\n",
      "\n",
      "Interval 314 (3130000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -4.2000\n",
      "42 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 228.586 - mean_q: -107.464 - mean_eps: 0.022 - score: 2.347\n",
      "\n",
      "Interval 315 (3140000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -4.6000\n",
      "46 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 212.697 - mean_q: -107.629 - mean_eps: 0.021 - score: 1.859\n",
      "\n",
      "Interval 316 (3150000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: -5.3000\n",
      "53 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 203.236 - mean_q: -106.624 - mean_eps: 0.021 - score: 1.111\n",
      "\n",
      "Interval 317 (3160000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -4.6000\n",
      "46 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 183.694 - mean_q: -107.983 - mean_eps: 0.021 - score: 2.374\n",
      "\n",
      "Interval 318 (3170000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -4.6000\n",
      "46 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 209.222 - mean_q: -111.039 - mean_eps: 0.021 - score: 1.768\n",
      "\n",
      "Interval 319 (3180000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -5.1000\n",
      "51 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 221.727 - mean_q: -110.011 - mean_eps: 0.020 - score: 1.146\n",
      "\n",
      "Interval 320 (3190000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -4.0000\n",
      "40 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 216.933 - mean_q: -109.648 - mean_eps: 0.020 - score: 3.552\n",
      "\n",
      "Interval 321 (3200000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -5.1000\n",
      "51 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 207.166 - mean_q: -107.773 - mean_eps: 0.020 - score: 2.568\n",
      "\n",
      "Interval 322 (3210000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -4.1000\n",
      "41 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 250.668 - mean_q: -106.351 - mean_eps: 0.020 - score: 3.389\n",
      "\n",
      "Interval 323 (3220000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -3.7000\n",
      "37 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 246.315 - mean_q: -102.994 - mean_eps: 0.019 - score: 2.471\n",
      "\n",
      "Interval 324 (3230000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -3.4000\n",
      "34 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 239.852 - mean_q: -101.544 - mean_eps: 0.019 - score: 4.607\n",
      "\n",
      "Interval 325 (3240000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -3.8000\n",
      "38 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 270.093 - mean_q: -98.865 - mean_eps: 0.019 - score: 2.657\n",
      "\n",
      "Interval 326 (3250000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -4.4000\n",
      "44 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 266.109 - mean_q: -96.341 - mean_eps: 0.019 - score: 1.878\n",
      "\n",
      "Interval 327 (3260000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: -4.5000\n",
      "45 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 249.660 - mean_q: -95.001 - mean_eps: 0.018 - score: 2.094\n",
      "\n",
      "Interval 328 (3270000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 82s 8ms/step - reward: -3.9000\n",
      "39 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 270.812 - mean_q: -93.662 - mean_eps: 0.018 - score: 2.439\n",
      "\n",
      "Interval 329 (3280000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -3.8000\n",
      "38 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 275.000 - mean_q: -92.586 - mean_eps: 0.018 - score: 3.153\n",
      "\n",
      "Interval 330 (3290000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -3.1000\n",
      "31 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 256.877 - mean_q: -92.147 - mean_eps: 0.018 - score: 4.958\n",
      "\n",
      "Interval 331 (3300000 steps performed)\n",
      "10000/10000 [==============================] - 83s 8ms/step - reward: -4.5000\n",
      "45 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 280.270 - mean_q: -90.696 - mean_eps: 0.017 - score: 2.261\n",
      "\n",
      "Interval 332 (3310000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: -3.9000\n",
      "39 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 291.291 - mean_q: -90.992 - mean_eps: 0.017 - score: 2.905\n",
      "\n",
      "Interval 333 (3320000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -3.6000\n",
      "36 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 266.454 - mean_q: -90.134 - mean_eps: 0.017 - score: 3.878\n",
      "\n",
      "Interval 334 (3330000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -3.9000\n",
      "39 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 289.880 - mean_q: -87.865 - mean_eps: 0.017 - score: 2.905\n",
      "\n",
      "Interval 335 (3340000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -4.8000\n",
      "48 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 250.031 - mean_q: -87.349 - mean_eps: 0.016 - score: 2.148\n",
      "\n",
      "Interval 336 (3350000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -4.7000\n",
      "47 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 237.186 - mean_q: -86.008 - mean_eps: 0.016 - score: 2.079\n",
      "\n",
      "Interval 337 (3360000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -4.6000\n",
      "46 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 230.608 - mean_q: -83.002 - mean_eps: 0.016 - score: 1.986\n",
      "\n",
      "Interval 338 (3370000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -3.3000\n",
      "33 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 290.561 - mean_q: -78.226 - mean_eps: 0.016 - score: 3.539\n",
      "\n",
      "Interval 339 (3380000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -3.3000\n",
      "33 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 249.660 - mean_q: -75.276 - mean_eps: 0.015 - score: 4.324\n",
      "\n",
      "Interval 340 (3390000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -3.9000\n",
      "39 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 233.161 - mean_q: -73.743 - mean_eps: 0.015 - score: 2.706\n",
      "\n",
      "Interval 341 (3400000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -2.9000\n",
      "29 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 237.184 - mean_q: -75.775 - mean_eps: 0.015 - score: 4.894\n",
      "\n",
      "Interval 342 (3410000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -2.6000\n",
      "26 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 225.870 - mean_q: -74.686 - mean_eps: 0.015 - score: 6.175\n",
      "\n",
      "Interval 343 (3420000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -3.3000\n",
      "33 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 257.831 - mean_q: -74.578 - mean_eps: 0.014 - score: 4.011\n",
      "\n",
      "Interval 344 (3430000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -2.9000\n",
      "29 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 209.671 - mean_q: -71.688 - mean_eps: 0.014 - score: 5.737\n",
      "\n",
      "Interval 345 (3440000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -2.9000\n",
      "29 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 211.230 - mean_q: -71.393 - mean_eps: 0.014 - score: 4.000\n",
      "\n",
      "Interval 346 (3450000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -3.4000\n",
      "34 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 217.349 - mean_q: -67.666 - mean_eps: 0.014 - score: 2.921\n",
      "\n",
      "Interval 347 (3460000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -3.8000\n",
      "38 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 250.927 - mean_q: -70.843 - mean_eps: 0.013 - score: 2.773\n",
      "\n",
      "Interval 348 (3470000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -3.7000\n",
      "37 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 219.553 - mean_q: -69.760 - mean_eps: 0.013 - score: 3.732\n",
      "\n",
      "Interval 349 (3480000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -3.9000\n",
      "39 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 283.428 - mean_q: -69.011 - mean_eps: 0.013 - score: 4.177\n",
      "\n",
      "Interval 350 (3490000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -3.6000\n",
      "36 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 272.342 - mean_q: -66.631 - mean_eps: 0.013 - score: 3.974\n",
      "\n",
      "Interval 351 (3500000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -4.2000\n",
      "42 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 326.266 - mean_q: -67.041 - mean_eps: 0.012 - score: 3.439\n",
      "\n",
      "Interval 352 (3510000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -4.2000\n",
      "42 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 360.347 - mean_q: -64.614 - mean_eps: 0.012 - score: 2.963\n",
      "\n",
      "Interval 353 (3520000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -3.1000\n",
      "31 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 344.598 - mean_q: -63.926 - mean_eps: 0.012 - score: 4.411\n",
      "\n",
      "Interval 354 (3530000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -2.5000\n",
      "25 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 317.361 - mean_q: -64.444 - mean_eps: 0.012 - score: 6.919\n",
      "\n",
      "Interval 355 (3540000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -2.6000\n",
      "26 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 291.137 - mean_q: -62.901 - mean_eps: 0.011 - score: 6.750\n",
      "\n",
      "Interval 356 (3550000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -2.8000\n",
      "28 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 329.390 - mean_q: -59.871 - mean_eps: 0.011 - score: 4.323\n",
      "\n",
      "Interval 357 (3560000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -3.7000\n",
      "37 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 327.713 - mean_q: -60.900 - mean_eps: 0.011 - score: 3.869\n",
      "\n",
      "Interval 358 (3570000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -3.3000\n",
      "33 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 300.622 - mean_q: -61.829 - mean_eps: 0.011 - score: 3.300\n",
      "\n",
      "Interval 359 (3580000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -3.4000\n",
      "34 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 298.992 - mean_q: -62.866 - mean_eps: 0.010 - score: 4.623\n",
      "\n",
      "Interval 360 (3590000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -3.1000\n",
      "31 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 323.320 - mean_q: -63.642 - mean_eps: 0.010 - score: 5.144\n",
      "\n",
      "Interval 361 (3600000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -3.4000\n",
      "34 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 263.740 - mean_q: -65.835 - mean_eps: 0.010 - score: 2.953\n",
      "\n",
      "Interval 362 (3610000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -3.1000\n",
      "31 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 311.775 - mean_q: -65.928 - mean_eps: 0.010 - score: 4.255\n",
      "\n",
      "Interval 363 (3620000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -3.5000\n",
      "35 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 282.801 - mean_q: -64.694 - mean_eps: 0.009 - score: 3.536\n",
      "\n",
      "Interval 364 (3630000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -3.0000\n",
      "30 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 331.788 - mean_q: -64.800 - mean_eps: 0.009 - score: 4.051\n",
      "\n",
      "Interval 365 (3640000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -2.3000\n",
      "23 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 370.256 - mean_q: -64.872 - mean_eps: 0.009 - score: 6.803\n",
      "\n",
      "Interval 366 (3650000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: -2.7000\n",
      "27 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 304.579 - mean_q: -65.798 - mean_eps: 0.009 - score: 5.760\n",
      "\n",
      "Interval 367 (3660000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -2.8000\n",
      "28 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 336.462 - mean_q: -67.468 - mean_eps: 0.008 - score: 10.894\n",
      "\n",
      "Interval 368 (3670000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -2.9000\n",
      "29 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 307.317 - mean_q: -66.549 - mean_eps: 0.008 - score: 7.213\n",
      "\n",
      "Interval 369 (3680000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -3.9000\n",
      "39 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 358.994 - mean_q: -67.465 - mean_eps: 0.008 - score: 6.241\n",
      "\n",
      "Interval 370 (3690000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -1.9000\n",
      "19 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 352.025 - mean_q: -67.533 - mean_eps: 0.008 - score: 8.557\n",
      "\n",
      "Interval 371 (3700000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -2.2000\n",
      "22 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 363.236 - mean_q: -65.977 - mean_eps: 0.007 - score: 7.640\n",
      "\n",
      "Interval 372 (3710000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -3.5000\n",
      "35 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 360.230 - mean_q: -62.735 - mean_eps: 0.007 - score: 6.531\n",
      "\n",
      "Interval 373 (3720000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -1.9000\n",
      "19 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 369.643 - mean_q: -60.761 - mean_eps: 0.007 - score: 7.177\n",
      "\n",
      "Interval 374 (3730000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: -2.3000\n",
      "23 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 332.173 - mean_q: -56.750 - mean_eps: 0.007 - score: 6.067\n",
      "\n",
      "Interval 375 (3740000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -1.4000\n",
      "14 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 310.458 - mean_q: -54.794 - mean_eps: 0.006 - score: 9.805\n",
      "\n",
      "Interval 376 (3750000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -2.2000\n",
      "22 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 275.474 - mean_q: -56.972 - mean_eps: 0.006 - score: 8.990\n",
      "\n",
      "Interval 377 (3760000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -2.5000\n",
      "25 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 267.236 - mean_q: -55.890 - mean_eps: 0.006 - score: 6.637\n",
      "\n",
      "Interval 378 (3770000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -1.0000\n",
      "10 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 270.221 - mean_q: -55.456 - mean_eps: 0.006 - score: 16.993\n",
      "\n",
      "Interval 379 (3780000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -2.0000\n",
      "20 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 226.796 - mean_q: -54.759 - mean_eps: 0.005 - score: 7.979\n",
      "\n",
      "Interval 380 (3790000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -2.0000\n",
      "20 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 247.032 - mean_q: -54.655 - mean_eps: 0.005 - score: 10.395\n",
      "\n",
      "Interval 381 (3800000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -1.0000\n",
      "10 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 246.706 - mean_q: -52.126 - mean_eps: 0.005 - score: 19.377\n",
      "\n",
      "Interval 382 (3810000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: -1.2000\n",
      "12 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 240.847 - mean_q: -49.776 - mean_eps: 0.005 - score: 12.935\n",
      "\n",
      "Interval 383 (3820000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -1.5000\n",
      "15 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 225.635 - mean_q: -49.962 - mean_eps: 0.004 - score: 17.948\n",
      "\n",
      "Interval 384 (3830000 steps performed)\n",
      "10000/10000 [==============================] - 69s 7ms/step - reward: -1.6000\n",
      "16 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 290.021 - mean_q: -53.862 - mean_eps: 0.004 - score: 11.138\n",
      "\n",
      "Interval 385 (3840000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -2.0000\n",
      "20 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 236.912 - mean_q: -53.666 - mean_eps: 0.004 - score: 7.295\n",
      "\n",
      "Interval 386 (3850000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -0.2000\n",
      "2 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 270.482 - mean_q: -54.097 - mean_eps: 0.004 - score: 57.905\n",
      "\n",
      "Interval 387 (3860000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: -1.2000\n",
      "12 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 230.892 - mean_q: -51.976 - mean_eps: 0.003 - score: 15.093\n",
      "\n",
      "Interval 388 (3870000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -1.0000\n",
      "10 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 232.523 - mean_q: -52.097 - mean_eps: 0.003 - score: 22.692\n",
      "\n",
      "Interval 389 (3880000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -1.0000\n",
      "10 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 193.491 - mean_q: -51.441 - mean_eps: 0.003 - score: 18.588\n",
      "\n",
      "Interval 390 (3890000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -1.0000\n",
      "10 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 184.624 - mean_q: -49.580 - mean_eps: 0.003 - score: 15.252\n",
      "\n",
      "Interval 391 (3900000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: -5.6000\n",
      "56 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 233.185 - mean_q: -49.854 - mean_eps: 0.002 - score: 2.861\n",
      "\n",
      "Interval 392 (3910000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -1.1000\n",
      "11 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 286.292 - mean_q: -49.725 - mean_eps: 0.002 - score: 18.953\n",
      "\n",
      "Interval 393 (3920000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -0.6000\n",
      "6 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 280.120 - mean_q: -50.254 - mean_eps: 0.002 - score: 31.028\n",
      "\n",
      "Interval 394 (3930000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 69s 7ms/step - reward: -0.4000\n",
      "4 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 280.669 - mean_q: -48.945 - mean_eps: 0.002 - score: 36.922\n",
      "\n",
      "Interval 395 (3940000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: -1.2000\n",
      "12 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 242.341 - mean_q: -48.263 - mean_eps: 0.001 - score: 14.806\n",
      "\n",
      "Interval 396 (3950000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -1.2000\n",
      "12 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 220.471 - mean_q: -47.041 - mean_eps: 0.001 - score: 22.562\n",
      "\n",
      "Interval 397 (3960000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 0.0000e+00\n",
      "Interval 398 (3970000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 0.0000e+00\n",
      "Interval 399 (3980000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -0.2000\n",
      "2 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 189.684 - mean_q: -44.541 - mean_eps: 0.000 - score: 151.053\n",
      "\n",
      "Interval 400 (3990000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -0.3000\n",
      "3 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 174.486 - mean_q: -43.509 - mean_eps: 0.000 - score: 314.339\n",
      "\n",
      "Interval 401 (4000000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -2.2000\n",
      "22 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 161.950 - mean_q: -42.369 - mean_eps: 0.000 - score: 33.579\n",
      "\n",
      "Interval 402 (4010000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -3.6000\n",
      "36 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 239.329 - mean_q: -41.566 - mean_eps: 0.000 - score: 14.905\n",
      "\n",
      "Interval 403 (4020000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -0.5000\n",
      "5 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 177.446 - mean_q: -39.615 - mean_eps: 0.000 - score: 34.993\n",
      "\n",
      "Interval 404 (4030000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -0.1000\n",
      "1 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 185.202 - mean_q: -39.057 - mean_eps: 0.000 - score: 125.720\n",
      "\n",
      "Interval 405 (4040000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: -1.0000\n",
      "10 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 180.319 - mean_q: -37.678 - mean_eps: 0.000 - score: 28.420\n",
      "\n",
      "Interval 406 (4050000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: -1.0000\n",
      "10 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 252.397 - mean_q: -36.052 - mean_eps: 0.000 - score: 24.155\n",
      "\n",
      "Interval 407 (4060000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -1.1000\n",
      "11 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 194.186 - mean_q: -33.469 - mean_eps: 0.000 - score: 12.796\n",
      "\n",
      "Interval 408 (4070000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -1.0000\n",
      "10 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 208.410 - mean_q: -32.419 - mean_eps: 0.000 - score: 18.651\n",
      "\n",
      "Interval 409 (4080000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -1.1000\n",
      "11 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 221.170 - mean_q: -32.029 - mean_eps: 0.000 - score: 17.425\n",
      "\n",
      "Interval 410 (4090000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -0.7000\n",
      "7 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 236.251 - mean_q: -32.606 - mean_eps: 0.000 - score: 23.353\n",
      "\n",
      "Interval 411 (4100000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -1.3000\n",
      "13 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 205.857 - mean_q: -33.080 - mean_eps: 0.000 - score: 14.818\n",
      "\n",
      "Interval 412 (4110000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: -0.7000\n",
      "7 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 186.197 - mean_q: -35.355 - mean_eps: 0.000 - score: 26.704\n",
      "\n",
      "Interval 413 (4120000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -0.8000\n",
      "8 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 207.693 - mean_q: -35.859 - mean_eps: 0.000 - score: 23.039\n",
      "\n",
      "Interval 414 (4130000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -0.8000\n",
      "8 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 213.248 - mean_q: -36.792 - mean_eps: 0.000 - score: 35.625\n",
      "\n",
      "Interval 415 (4140000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -1.7000\n",
      "17 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 252.113 - mean_q: -37.505 - mean_eps: 0.000 - score: 14.465\n",
      "\n",
      "Interval 416 (4150000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -3.0000\n",
      "30 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 328.824 - mean_q: -35.411 - mean_eps: 0.000 - score: 6.474\n",
      "\n",
      "Interval 417 (4160000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: -0.9000\n",
      "9 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 302.936 - mean_q: -39.282 - mean_eps: 0.000 - score: 13.642\n",
      "\n",
      "Interval 418 (4170000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -0.7000\n",
      "7 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 256.413 - mean_q: -39.548 - mean_eps: 0.000 - score: 18.708\n",
      "\n",
      "Interval 419 (4180000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -1.0000\n",
      "10 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 273.197 - mean_q: -38.455 - mean_eps: 0.000 - score: 23.238\n",
      "\n",
      "Interval 420 (4190000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -2.0000\n",
      "20 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 287.186 - mean_q: -37.295 - mean_eps: 0.000 - score: 9.393\n",
      "\n",
      "Interval 421 (4200000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -2.5000\n",
      "25 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 359.950 - mean_q: -35.505 - mean_eps: 0.000 - score: 5.406\n",
      "\n",
      "Interval 422 (4210000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -0.9000\n",
      "9 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 298.126 - mean_q: -34.995 - mean_eps: 0.000 - score: 24.336\n",
      "\n",
      "Interval 423 (4220000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -1.2000\n",
      "12 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 333.489 - mean_q: -34.675 - mean_eps: 0.000 - score: 21.018\n",
      "\n",
      "Interval 424 (4230000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -0.8000\n",
      "8 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 341.306 - mean_q: -37.384 - mean_eps: 0.000 - score: 49.890\n",
      "\n",
      "Interval 425 (4240000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -0.3000\n",
      "3 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 330.838 - mean_q: -35.666 - mean_eps: 0.000 - score: 53.777\n",
      "\n",
      "Interval 426 (4250000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -0.8000\n",
      "8 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 264.851 - mean_q: -36.390 - mean_eps: 0.000 - score: 33.396\n",
      "\n",
      "Interval 427 (4260000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -1.4000\n",
      "14 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 337.280 - mean_q: -35.534 - mean_eps: 0.000 - score: 50.743\n",
      "\n",
      "Interval 428 (4270000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 55s 5ms/step - reward: -0.7000\n",
      "7 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 322.316 - mean_q: -36.828 - mean_eps: 0.000 - score: 45.043\n",
      "\n",
      "Interval 429 (4280000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -0.6000\n",
      "6 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 285.728 - mean_q: -35.583 - mean_eps: 0.000 - score: 28.752\n",
      "\n",
      "Interval 430 (4290000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -1.3000\n",
      "13 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 295.377 - mean_q: -35.413 - mean_eps: 0.000 - score: 23.183\n",
      "\n",
      "Interval 431 (4300000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -1.1000\n",
      "11 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 258.146 - mean_q: -38.424 - mean_eps: 0.000 - score: 22.590\n",
      "\n",
      "Interval 432 (4310000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -0.9000\n",
      "9 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 236.909 - mean_q: -38.347 - mean_eps: 0.000 - score: 34.269\n",
      "\n",
      "Interval 433 (4320000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -1.4000\n",
      "14 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 277.814 - mean_q: -37.087 - mean_eps: 0.000 - score: 12.101\n",
      "\n",
      "Interval 434 (4330000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -1.8000\n",
      "18 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 295.972 - mean_q: -35.451 - mean_eps: 0.000 - score: 7.456\n",
      "\n",
      "Interval 435 (4340000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: -1.6000\n",
      "16 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 327.569 - mean_q: -35.705 - mean_eps: 0.000 - score: 9.690\n",
      "\n",
      "Interval 436 (4350000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -1.1000\n",
      "11 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 312.925 - mean_q: -37.159 - mean_eps: 0.000 - score: 15.708\n",
      "\n",
      "Interval 437 (4360000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -1.2000\n",
      "12 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 336.297 - mean_q: -41.763 - mean_eps: 0.000 - score: 16.471\n",
      "\n",
      "Interval 438 (4370000 steps performed)\n",
      "10000/10000 [==============================] - 69s 7ms/step - reward: -0.2000\n",
      "2 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 343.393 - mean_q: -41.650 - mean_eps: 0.000 - score: 62.831\n",
      "\n",
      "Interval 439 (4380000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -0.6000\n",
      "6 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 336.983 - mean_q: -40.775 - mean_eps: 0.000 - score: 33.505\n",
      "\n",
      "Interval 440 (4390000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -1.3000\n",
      "13 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 329.126 - mean_q: -39.559 - mean_eps: 0.000 - score: 38.100\n",
      "\n",
      "Interval 441 (4400000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -0.3000\n",
      "3 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 316.756 - mean_q: -36.906 - mean_eps: 0.000 - score: 33.746\n",
      "\n",
      "Interval 442 (4410000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -0.4000\n",
      "4 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 266.198 - mean_q: -36.262 - mean_eps: 0.000 - score: 56.583\n",
      "\n",
      "Interval 443 (4420000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -1.0000\n",
      "10 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 245.999 - mean_q: -35.407 - mean_eps: 0.000 - score: 19.131\n",
      "\n",
      "Interval 444 (4430000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -0.8000\n",
      "8 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 276.743 - mean_q: -35.634 - mean_eps: 0.000 - score: 28.335\n",
      "\n",
      "Interval 445 (4440000 steps performed)\n",
      "10000/10000 [==============================] - 75s 7ms/step - reward: -1.9000\n",
      "19 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 254.755 - mean_q: -35.307 - mean_eps: 0.000 - score: 17.429\n",
      "\n",
      "Interval 446 (4450000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -0.7000\n",
      "7 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 225.430 - mean_q: -35.111 - mean_eps: 0.000 - score: 49.311\n",
      "\n",
      "Interval 447 (4460000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -1.5000\n",
      "15 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 302.938 - mean_q: -32.032 - mean_eps: 0.000 - score: 20.295\n",
      "\n",
      "Interval 448 (4470000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -1.3000\n",
      "13 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 250.606 - mean_q: -29.135 - mean_eps: 0.000 - score: 18.822\n",
      "\n",
      "Interval 449 (4480000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -2.2000\n",
      "22 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 287.356 - mean_q: -27.990 - mean_eps: 0.000 - score: 8.872\n",
      "\n",
      "Interval 450 (4490000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -0.9000\n",
      "9 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 321.236 - mean_q: -28.147 - mean_eps: 0.000 - score: 20.362\n",
      "\n",
      "Interval 451 (4500000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -0.1000\n",
      "1 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 276.652 - mean_q: -29.606 - mean_eps: 0.000 - score: 79.727\n",
      "\n",
      "Interval 452 (4510000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -0.9000\n",
      "9 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 276.602 - mean_q: -27.653 - mean_eps: 0.000 - score: 34.605\n",
      "\n",
      "Interval 453 (4520000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -2.3000\n",
      "23 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 278.713 - mean_q: -26.626 - mean_eps: 0.000 - score: 11.682\n",
      "\n",
      "Interval 454 (4530000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -2.2000\n",
      "22 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 287.328 - mean_q: -25.005 - mean_eps: 0.000 - score: 13.122\n",
      "\n",
      "Interval 455 (4540000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: -0.8000\n",
      "8 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 318.682 - mean_q: -24.865 - mean_eps: 0.000 - score: 31.971\n",
      "\n",
      "Interval 456 (4550000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -1.3000\n",
      "13 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 307.146 - mean_q: -24.712 - mean_eps: 0.000 - score: 12.744\n",
      "\n",
      "Interval 457 (4560000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: -1.0000\n",
      "10 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 312.059 - mean_q: -25.485 - mean_eps: 0.000 - score: 22.958\n",
      "\n",
      "Interval 458 (4570000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -0.7000\n",
      "7 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 277.629 - mean_q: -24.945 - mean_eps: 0.000 - score: 31.859\n",
      "\n",
      "Interval 459 (4580000 steps performed)\n",
      " 5689/10000 [================>.............] - ETA: 29s - reward: -1.054"
     ]
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=8000000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3aa6799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving weights of Neural Network\n",
    "#dqn.save_weights(\"weights/flappy_bird_solution_8million.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eac34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights(\"weights/flappy_bird_solution_8million.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd71500",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dqn.test(env, visualize=False, nb_episodes=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec183a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6081.77\n",
      "5665.289834259287\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.mean(results.history['nb_steps']))\n",
    "print(np.sqrt(np.cov(results.history['nb_steps'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315e386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
