{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf18f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import flappy_bird_gym\n",
    "import gymnasium\n",
    "env = flappy_bird_gym.make(\"FlappyBird-12-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89606d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "def build_model(obs, actions):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(64, activation='relu', input_shape=(1, obs)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435fcd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444c85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22ba89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=0.1, value_min=.0001, value_test=.0, nb_steps=1000000)\n",
    "    memory = SequentialMemory(limit=100000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                enable_dueling_network=True, dueling_type='avg',\n",
    "                nb_actions=actions, nb_steps_warmup=500)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f896e74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1, 64)             832       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 128)            8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 256)            33024     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1, 64)             16448     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1, 128)            8320      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,202\n",
      "Trainable params: 67,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(obs, actions)\n",
    "dqn = build_agent(model, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317f4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7afb3cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "#Training the Neural Network\n",
    "dqn.compile(Adam(learning_rate=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41620de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -12.2000\n",
      "122 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 5413.114 - mean_q: -10.422 - mean_eps: 0.099 - score: 0.001\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -10.3000\n",
      "103 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 3372.506 - mean_q: -18.352 - mean_eps: 0.099 - score: 0.000\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 1485.526 - mean_q: -24.667 - mean_eps: 0.098 - score: 0.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 474.388 - mean_q: -32.852 - mean_eps: 0.097 - score: 0.000\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -10.0000\n",
      "100 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 323.048 - mean_q: -38.681 - mean_eps: 0.096 - score: 0.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 240.212 - mean_q: -51.060 - mean_eps: 0.095 - score: 0.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 194.782 - mean_q: -53.940 - mean_eps: 0.094 - score: 0.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 145.556 - mean_q: -60.497 - mean_eps: 0.093 - score: 0.000\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 125.737 - mean_q: -66.396 - mean_eps: 0.092 - score: 0.000\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 117.643 - mean_q: -75.157 - mean_eps: 0.091 - score: 0.000\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: -9.7000\n",
      "97 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 96.461 - mean_q: -84.571 - mean_eps: 0.090 - score: 0.000\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 55.998 - mean_q: -94.452 - mean_eps: 0.089 - score: 0.000\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -9.7000\n",
      "97 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 49.961 - mean_q: -102.781 - mean_eps: 0.088 - score: 0.001\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -10.0000\n",
      "100 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 55.230 - mean_q: -112.075 - mean_eps: 0.087 - score: 0.000\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 79.441 - mean_q: -121.948 - mean_eps: 0.086 - score: 0.001\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "  329/10000 [..............................] - ETA: 1:14 - reward: -9.1185"
     ]
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=1000000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3aa6799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving weights of Neural Network\n",
    "#dqn.save_weights(\"weights/flappy_bird_solution_12.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a91a6d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights(\"weights/flappy_bird_solution_12.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af9d7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dqn.test(env, visualize=False, nb_episodes=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dd71500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822.71\n",
      "642.1313829975164\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.mean(results.history['nb_steps']))\n",
    "print(np.sqrt(np.cov(results.history['nb_steps'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8022b5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
