{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf18f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import flappy_bird_gym\n",
    "import gymnasium\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89606d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "def build_model(obs, actions):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(64, activation='relu', input_shape=(1, obs)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435fcd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444c85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22ba89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=0.1, value_min=.0001, value_test=.0, nb_steps=1000000)\n",
    "    memory = SequentialMemory(limit=100000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                enable_dueling_network=True, dueling_type='avg',\n",
    "                nb_actions=actions, nb_steps_warmup=500)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f896e74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1, 64)             192       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 128)            8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 256)            33024     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1, 64)             16448     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1, 128)            8320      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,562\n",
      "Trainable params: 66,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(obs, actions)\n",
    "dqn = build_agent(model, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317f4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7afb3cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "#Training the Neural Network\n",
    "dqn.compile(Adam(learning_rate=0.00025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41620de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 46s 5ms/step - reward: -10.1000\n",
      "101 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 3028.011 - mean_q: -7.578 - mean_eps: 0.099 - score: 0.000\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 49s 5ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 248.466 - mean_q: -18.711 - mean_eps: 0.099 - score: 0.000\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 148.769 - mean_q: -26.409 - mean_eps: 0.098 - score: 0.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 122.113 - mean_q: -36.216 - mean_eps: 0.097 - score: 0.000\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 108.734 - mean_q: -42.483 - mean_eps: 0.096 - score: 0.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 69.600 - mean_q: -51.369 - mean_eps: 0.095 - score: 0.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 60.186 - mean_q: -60.198 - mean_eps: 0.094 - score: 0.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 40.987 - mean_q: -69.548 - mean_eps: 0.093 - score: 0.000\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 53.189 - mean_q: -78.682 - mean_eps: 0.092 - score: 0.000\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 29.480 - mean_q: -87.465 - mean_eps: 0.091 - score: 0.000\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 16.884 - mean_q: -96.884 - mean_eps: 0.090 - score: 0.000\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 11.019 - mean_q: -105.080 - mean_eps: 0.089 - score: 0.000\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 10.264 - mean_q: -114.429 - mean_eps: 0.088 - score: 0.000\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 7.528 - mean_q: -122.990 - mean_eps: 0.087 - score: 0.000\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 5.851 - mean_q: -131.458 - mean_eps: 0.086 - score: 0.000\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 8.199 - mean_q: -139.367 - mean_eps: 0.085 - score: 0.000\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 9.231 - mean_q: -148.837 - mean_eps: 0.084 - score: 0.000\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 8.702 - mean_q: -156.126 - mean_eps: 0.083 - score: 0.000\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 9.777 - mean_q: -164.546 - mean_eps: 0.082 - score: 0.000\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 8.302 - mean_q: -173.281 - mean_eps: 0.081 - score: 0.000\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 8.539 - mean_q: -179.485 - mean_eps: 0.080 - score: 0.000\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 8.956 - mean_q: -188.872 - mean_eps: 0.079 - score: 0.000\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 8.902 - mean_q: -195.897 - mean_eps: 0.078 - score: 0.000\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 6.686 - mean_q: -202.245 - mean_eps: 0.077 - score: 0.000\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 11.458 - mean_q: -211.320 - mean_eps: 0.076 - score: 0.000\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 6.788 - mean_q: -217.795 - mean_eps: 0.075 - score: 0.000\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 10.325 - mean_q: -226.065 - mean_eps: 0.074 - score: 0.000\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 12.113 - mean_q: -235.288 - mean_eps: 0.073 - score: 0.000\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 7.502 - mean_q: -243.148 - mean_eps: 0.072 - score: 0.000\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 14.949 - mean_q: -250.183 - mean_eps: 0.071 - score: 0.000\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 8.052 - mean_q: -257.714 - mean_eps: 0.070 - score: 0.001\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 4.891 - mean_q: -266.264 - mean_eps: 0.069 - score: 0.000\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 4.989 - mean_q: -273.080 - mean_eps: 0.068 - score: 0.000\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 9.272 - mean_q: -281.637 - mean_eps: 0.067 - score: 0.000\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 6.371 - mean_q: -287.982 - mean_eps: 0.066 - score: 0.000\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 7.800 - mean_q: -295.656 - mean_eps: 0.065 - score: 0.001\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 5.482 - mean_q: -301.590 - mean_eps: 0.064 - score: 0.000\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 8.160 - mean_q: -310.759 - mean_eps: 0.063 - score: 0.001\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 8.801 - mean_q: -316.854 - mean_eps: 0.062 - score: 0.000\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 4.619 - mean_q: -322.284 - mean_eps: 0.061 - score: 0.000\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 5.209 - mean_q: -327.003 - mean_eps: 0.060 - score: 0.000\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 9.734 - mean_q: -335.179 - mean_eps: 0.059 - score: 0.005\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 6.811 - mean_q: -342.363 - mean_eps: 0.058 - score: 0.001\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 7.827 - mean_q: -348.225 - mean_eps: 0.057 - score: 0.001\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 6.290 - mean_q: -353.260 - mean_eps: 0.056 - score: 0.003\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.7000\n",
      "97 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 19.745 - mean_q: -360.765 - mean_eps: 0.055 - score: 0.008\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 6.036 - mean_q: -366.884 - mean_eps: 0.054 - score: 0.000\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 5.423 - mean_q: -375.049 - mean_eps: 0.053 - score: 0.003\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 12.122 - mean_q: -379.492 - mean_eps: 0.052 - score: 0.000\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 7.472 - mean_q: -385.468 - mean_eps: 0.051 - score: 0.001\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 8.017 - mean_q: -391.625 - mean_eps: 0.050 - score: 0.004\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 16.848 - mean_q: -397.908 - mean_eps: 0.049 - score: 0.003\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 21.364 - mean_q: -391.890 - mean_eps: 0.048 - score: 0.003\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 15.682 - mean_q: -395.465 - mean_eps: 0.047 - score: 0.004\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 15.941 - mean_q: -402.078 - mean_eps: 0.046 - score: 0.002\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 11.685 - mean_q: -406.492 - mean_eps: 0.045 - score: 0.005\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 14.883 - mean_q: -410.735 - mean_eps: 0.044 - score: 0.006\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.7000\n",
      "97 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 21.170 - mean_q: -418.595 - mean_eps: 0.043 - score: 0.017\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.7000\n",
      "97 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 19.731 - mean_q: -423.474 - mean_eps: 0.042 - score: 0.010\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 19.459 - mean_q: -427.297 - mean_eps: 0.041 - score: 0.003\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.8000\n",
      "98 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 15.540 - mean_q: -431.278 - mean_eps: 0.040 - score: 0.005\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -9.5000\n",
      "95 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 18.532 - mean_q: -437.847 - mean_eps: 0.039 - score: 0.030\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.5000\n",
      "95 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 15.619 - mean_q: -441.671 - mean_eps: 0.038 - score: 0.019\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.3000\n",
      "93 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 19.902 - mean_q: -448.773 - mean_eps: 0.037 - score: 0.063\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.3000\n",
      "93 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 23.708 - mean_q: -455.769 - mean_eps: 0.036 - score: 0.055\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -8.8000\n",
      "88 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 25.989 - mean_q: -460.249 - mean_eps: 0.035 - score: 0.146\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.2000\n",
      "92 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 26.120 - mean_q: -466.095 - mean_eps: 0.034 - score: 0.082\n",
      "\n",
      "Interval 68 (670000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.3000\n",
      "93 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 30.279 - mean_q: -473.952 - mean_eps: 0.033 - score: 0.046\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.3000\n",
      "93 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 27.180 - mean_q: -480.868 - mean_eps: 0.032 - score: 0.036\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -9.3000\n",
      "93 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 33.487 - mean_q: -482.686 - mean_eps: 0.031 - score: 0.056\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -8.6000\n",
      "86 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 35.233 - mean_q: -490.804 - mean_eps: 0.030 - score: 0.170\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -7.8000\n",
      "78 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 38.037 - mean_q: -499.718 - mean_eps: 0.029 - score: 0.311\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -8.2000\n",
      "82 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 46.988 - mean_q: -506.484 - mean_eps: 0.028 - score: 0.291\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -8.3000\n",
      "83 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 54.504 - mean_q: -515.087 - mean_eps: 0.027 - score: 0.241\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -7.7000\n",
      "77 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 60.433 - mean_q: -520.282 - mean_eps: 0.026 - score: 0.480\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -6.6000\n",
      "66 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 66.503 - mean_q: -529.922 - mean_eps: 0.025 - score: 0.768\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -7.0000\n",
      "70 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 68.416 - mean_q: -537.586 - mean_eps: 0.024 - score: 0.707\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -6.5000\n",
      "65 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 74.009 - mean_q: -544.523 - mean_eps: 0.023 - score: 1.042\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -6.5000\n",
      "65 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 88.907 - mean_q: -552.694 - mean_eps: 0.022 - score: 0.980\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -5.8000\n",
      "58 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 85.970 - mean_q: -560.020 - mean_eps: 0.021 - score: 1.082\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -7.7000\n",
      "77 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 90.736 - mean_q: -560.970 - mean_eps: 0.020 - score: 0.372\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -6.6000\n",
      "66 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 88.161 - mean_q: -564.040 - mean_eps: 0.019 - score: 0.729\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -6.5000\n",
      "65 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 93.142 - mean_q: -567.151 - mean_eps: 0.018 - score: 1.029\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -6.5000\n",
      "65 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 98.912 - mean_q: -567.125 - mean_eps: 0.017 - score: 0.935\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -7.5000\n",
      "75 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 104.509 - mean_q: -569.642 - mean_eps: 0.016 - score: 0.445\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -7.6000\n",
      "76 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 110.946 - mean_q: -567.214 - mean_eps: 0.015 - score: 0.462\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -7.2000\n",
      "72 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 118.134 - mean_q: -564.250 - mean_eps: 0.014 - score: 0.544\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -5.3000\n",
      "53 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 130.552 - mean_q: -561.132 - mean_eps: 0.013 - score: 2.444\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -5.6000\n",
      "56 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 122.566 - mean_q: -559.340 - mean_eps: 0.012 - score: 1.422\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -4.8000\n",
      "48 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 126.907 - mean_q: -554.020 - mean_eps: 0.011 - score: 2.267\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -5.1000\n",
      "51 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 135.478 - mean_q: -559.848 - mean_eps: 0.010 - score: 1.930\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -3.9000\n",
      "39 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 146.789 - mean_q: -564.469 - mean_eps: 0.009 - score: 2.296\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -4.4000\n",
      "44 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 153.910 - mean_q: -570.135 - mean_eps: 0.008 - score: 2.346\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -6.7000\n",
      "67 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 168.899 - mean_q: -567.329 - mean_eps: 0.007 - score: 0.818\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -8.3000\n",
      "83 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 192.961 - mean_q: -561.513 - mean_eps: 0.006 - score: 0.215\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -7.4000\n",
      "74 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 221.068 - mean_q: -560.110 - mean_eps: 0.005 - score: 0.543\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -6.3000\n",
      "63 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 231.661 - mean_q: -551.280 - mean_eps: 0.004 - score: 0.681\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -6.1000\n",
      "61 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 259.461 - mean_q: -547.429 - mean_eps: 0.003 - score: 0.857\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: -6.9000\n",
      "69 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 296.726 - mean_q: -542.870 - mean_eps: 0.002 - score: 0.535\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -6.3000\n",
      "done, took 5718.810 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fee4f39780>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=1000000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3aa6799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving weights of Neural Network\n",
    "#dqn.save_weights(\"weights/flappy_bird_solution_simple.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd71500",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights(\"weights/flappy_bird_solution_simple.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19b1651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dqn.test(env, visualize=False, nb_episodes=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85f0ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.65\n",
      "48.39387535336648\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.mean(results.history['nb_steps']))\n",
    "print(np.sqrt(np.cov(results.history['nb_steps'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a908125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
