{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf18f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import flappy_bird_gym\n",
    "import gymnasium\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89606d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "def build_model(obs, actions):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(64, activation='relu', input_shape=(1, obs)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435fcd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444c85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22ba89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=0.1, value_min=.0001, value_test=.0, nb_steps=1000000)\n",
    "    memory = SequentialMemory(limit=100000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                enable_dueling_network=True, dueling_type='avg',\n",
    "                nb_actions=actions, nb_steps_warmup=500)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f896e74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1, 64)             192       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 128)            8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 256)            33024     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1, 64)             16448     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1, 128)            8320      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,562\n",
      "Trainable params: 66,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(obs, actions)\n",
    "dqn = build_agent(model, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317f4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7afb3cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "#Training the Neural Network\n",
    "dqn.compile(Adam(learning_rate=0.00025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41620de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 2084.312 - mean_q: -7.283 - mean_eps: 0.099 - score: 0.003\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 390.276 - mean_q: -18.522 - mean_eps: 0.099 - score: 0.000\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 69s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 163.772 - mean_q: -27.776 - mean_eps: 0.098 - score: 0.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 182.910 - mean_q: -36.737 - mean_eps: 0.097 - score: 0.000\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 160.797 - mean_q: -28.958 - mean_eps: 0.096 - score: 0.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 146.110 - mean_q: -38.862 - mean_eps: 0.095 - score: 0.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 142.478 - mean_q: -48.221 - mean_eps: 0.094 - score: 0.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 136.787 - mean_q: -57.657 - mean_eps: 0.093 - score: 0.000\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 104.964 - mean_q: -66.060 - mean_eps: 0.092 - score: 0.000\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 77.755 - mean_q: -76.125 - mean_eps: 0.091 - score: 0.000\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 39582s 4s/step - reward: -9.7000\n",
      "97 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 61.945 - mean_q: -85.996 - mean_eps: 0.090 - score: 0.012\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -9.5000\n",
      "95 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 117.652 - mean_q: -94.983 - mean_eps: 0.089 - score: 0.034\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -9.7000\n",
      "97 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 150.734 - mean_q: -99.656 - mean_eps: 0.088 - score: 0.025\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -9.7000\n",
      "97 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 223.107 - mean_q: -102.710 - mean_eps: 0.087 - score: 0.058\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -10.0000\n",
      "100 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 271.404 - mean_q: -109.182 - mean_eps: 0.086 - score: 0.003\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 274.921 - mean_q: -118.439 - mean_eps: 0.085 - score: 0.000\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 285.880 - mean_q: -127.664 - mean_eps: 0.084 - score: 0.000\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: -9.9000\n",
      "99 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 319.873 - mean_q: -137.155 - mean_eps: 0.083 - score: 0.002\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -9.6000\n",
      "96 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 314.570 - mean_q: -149.188 - mean_eps: 0.082 - score: 0.020\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -9.2000\n",
      "92 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 334.453 - mean_q: -156.111 - mean_eps: 0.081 - score: 0.059\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -9.0000\n",
      "90 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 355.716 - mean_q: -159.263 - mean_eps: 0.080 - score: 0.100\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -8.9000\n",
      "89 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 431.510 - mean_q: -165.485 - mean_eps: 0.079 - score: 0.111\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -8.5000\n",
      "85 episodes - episode_reward: -1000.000 [-1000.000, -1000.000] - loss: 432.725 - mean_q: -163.731 - mean_eps: 0.078 - score: 0.153\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      " 1893/10000 [====>.........................] - ETA: 53s - reward: -7.395"
     ]
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=1000000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3aa6799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving weights of Neural Network\n",
    "#dqn.save_weights(\"weights/flappy_bird_solution_simple.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd71500",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights(\"weights/flappy_bird_solution_simple.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19b1651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dqn.test(env, visualize=False, nb_episodes=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85f0ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305.59\n",
      "166.31527168591467\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.mean(results.history['nb_steps']))\n",
    "print(np.sqrt(np.cov(results.history['nb_steps'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a908125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
